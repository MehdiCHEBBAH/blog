<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<link rel="stylesheet" href="../../css/blog-post.css" type="text/css">

<title>How To Choose The Right Embedding Model For You</title>
<meta name="description" content="This is a blog post that shows you how to choose the Embedding model that best fits your needs.">
<meta name="robots" content="index, follow">
<meta property="og:type" content="article" />
<meta property="og:title" content="How To Choose The Right Embedding Model For You" />
<meta property="og:description" content="This is a blog post that shows you how to choose the Embedding model that best fits your needs." />
<meta property="og:site_name" content="Mehdi CHEBBAH | Blog" />
</head>
<body class='typora-export'><div class='typora-export-content'>
<div id='write'  class=' first-line-indent'><div style="text-align:center;font-size:30px">﷽</div><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><div style="text-align:center;font-size:48px">How To Choose The Right Embedding Model For You</div><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p><span>Work done by </span><strong><span>Mehdi CHEBBAH</span></strong></p><hr /><p><img src="assets/Cover.png" referrerpolicy="no-referrer"></p><p><span>Embedding is a very famous task in NLP that means transforming the text from its natural format (Words and letters) to a format that could be used in ML/DL algorithms (Because these algorithms use numbers as inputs and outputs).</span></p><p><span>In this blog article, we will do a brief comparison between some Text Embedding methods. and learn how to decide which Embedding method is the best for our use case. Then we will be describing a small use-case of text embeddings (A Textual Similarity Calculator).</span></p><h1 id='comparison-criteria'><span>Comparison Criteria</span></h1><p><span>We could find in the literature lot of options for this task. In this article, I’ve made a comparison between some well-known Embedding methods. This comparison is based on some criteria that I judged important to decide what approach to use depending on your use case. For instance:</span></p><p><strong><span>Embeds</span></strong><span>: This criterion explains the level where the algorithm operates. It has 4 possible values:</span></p><ul><li><em><span>Character N-grams</span></em><span>: The algorithm operates on the characters’ level. This means that it takes a sequence of N characters as an input. This will allow the algorithm to learn not only the words in the corpus but also words out of vocabulary that could be derived from other words by taking only N chars of the word (For example in a 3-gram model: boyfriend –&gt; boy, oyf, yfr, fri, rie, ien, end)</span></li><li><em><span>Words</span></em><span>: The algorithm operates on the words level. This means that it learns the representation of words. This is useful in cases where we are trying to embed a few words, or if we are comparing them, or looking for synonyms, or other tasks related to words. But this type of embeddings could also be used to represent sentences, thanks to the characteristics of word vectors that hold information other than the meaning of the word. For example, if we take the embeddings of these words: King, Man, Woman, Queen we could Note that King — Man + Woman = Queen. This means that we could represent a sentence using the sum of word embeddings that compose this sentence.</span></li><li><em><span>Sub-words</span></em><span>: The algorithm operates on the sub-words level. This type was invented to overcome the out-of-vocabulary problem while not being too fine-grained and get all the N-Grams. (For example: Because –&gt; Because, cause, use OR boyfriend –&gt; Boyfriend, boy, friend, end).</span></li><li><em><span>Sentences</span></em><span>: The algorithm operates on sentences level. This will allow the algorithm not only to learn words, but also it allows to learn the context. This means a better understanding of the semantic of sentences.</span></li></ul><p><strong><span>Type</span></strong><span>: This criterion is available only for Words and sub-words operating levels. It has two modalities that have these meanings:</span></p><ul><li><em><span>Static</span></em><span>: If the embedding is static, that means that the model doesn’t take in consideration the context of the words. In other words, if the same word appeared in two different contexts it has the same vector. For example the word “Sort” in “I know the </span><strong><span>sort</span></strong><span> of people you’re mixing with” has the same representation as in “You have to </span><strong><span>sort</span></strong><span> the array”.</span></li><li><em><span>Dynamic</span></em><span>: The representation of the word -here- depends on the context (i.e If the same word appears in two different contexts it will have two different vector representations).</span></li></ul><p><strong><span>Pre-trained</span></strong><span>: This criterion indicates if the technique has pre-trained models that could be used directly or need a small fine-tuning before it can be used. If you choose to use a pre-trained model, that means that you don’t have to re-train a new one, this will save you a lot of time to focus on some other tasks in your business process.</span></p><p><strong><span>DL architecture</span></strong><span>: This criterion indicates the Deep Learning architecture used to solve the problem. The majority of new techniques are using Deep Learning Transformers as they offer the best results so far.</span></p><p><strong><span>Developed by</span></strong><span>: We mention here who invented the technique. And also the year of publication of it (In general recent methods are more accurate than old ones).</span></p><h1 id='comparison'><span>Comparison</span></h1><p><span>In the following Table and the paragraphs after I will explain in detail my comparison and mention the pros/cons of each technique.</span></p><figure><table><thead><tr><th><span>Method</span></th><th style='text-align:center;' ><span>Embeds</span></th><th style='text-align:center;' ><span>Type</span></th><th style='text-align:center;' ><span>Pre-trained</span></th><th style='text-align:center;' ><span>DL architecture</span></th><th style='text-align:center;' ><span>Developed by</span></th></tr></thead><tbody><tr><td><span>BOW</span></td><td style='text-align:center;' ><span>Words</span></td><td style='text-align:center;' ><span>Static</span></td><td style='text-align:center;' ><span>No</span></td><td style='text-align:center;' ><span>/</span></td><td style='text-align:center;' ><span>(Harris, 1954)</span></td></tr><tr><td><span>TF-IDF</span></td><td style='text-align:center;' ><span>Words</span></td><td style='text-align:center;' ><span>Static</span></td><td style='text-align:center;' ><span>No</span></td><td style='text-align:center;' ><span>/</span></td><td style='text-align:center;' ><span>(Jones, 1972)</span></td></tr><tr><td><span>Word2Vec (CBOW)</span></td><td style='text-align:center;' ><span>Words</span></td><td style='text-align:center;' ><span>Static</span></td><td style='text-align:center;' ><span>English</span></td><td style='text-align:center;' ><span>FNN</span></td><td style='text-align:center;' ><span>(Mikolov </span><em><span>et al.</span></em><span>, 2013)[Google]</span></td></tr><tr><td><span>Word2Vec (Skip-gram)</span></td><td style='text-align:center;' ><span>Words</span></td><td style='text-align:center;' ><span>Static</span></td><td style='text-align:center;' ><span>English</span></td><td style='text-align:center;' ><span>FNN</span></td><td style='text-align:center;' ><span>(Mikolov </span><em><span>et al.</span></em><span>, 2013) [Google]</span></td></tr><tr><td><span>GloVe</span></td><td style='text-align:center;' ><span>Words</span></td><td style='text-align:center;' ><span>Static</span></td><td style='text-align:center;' ><span>English</span></td><td style='text-align:center;' ><span>/</span></td><td style='text-align:center;' ><span>(Pennington, Socher and Manning, 2014)</span></td></tr><tr><td><span>Doc2Vec (Gensim)</span></td><td style='text-align:center;' ><span>Sentences</span></td><td style='text-align:center;' ><span>/</span></td><td style='text-align:center;' ><span>English</span></td><td style='text-align:center;' ><span>FNN</span></td><td style='text-align:center;' ><span>(Le and Mikolov, 2014) [Google]</span></td></tr><tr><td><span>Skip-Thought</span></td><td style='text-align:center;' ><span>Sentences</span></td><td style='text-align:center;' ><span>/</span></td><td style='text-align:center;' ><span>English</span></td><td style='text-align:center;' ><span>LSTM</span></td><td style='text-align:center;' ><span>(Kiros </span><em><span>et al.</span></em><span>, 2015)</span></td></tr><tr><td><span>Word2Vec (FastText)</span></td><td style='text-align:center;' ><span>character n-grams</span></td><td style='text-align:center;' ><span>/</span></td><td style='text-align:center;' ><span>Multiple languages</span></td><td style='text-align:center;' ><span>FNN</span></td><td style='text-align:center;' ><span>(Joulin </span><em><span>et al.</span></em><span>, 2016) [Facebook]</span></td></tr><tr><td><span>USE</span></td><td style='text-align:center;' ><span>Sentences</span></td><td style='text-align:center;' ><span>/</span></td><td style='text-align:center;' ><span>English</span></td><td style='text-align:center;' ><span>Transformers / DAN Encoders</span></td><td style='text-align:center;' ><span>(Cer </span><em><span>et al.</span></em><span>, 2018) [Google]</span></td></tr><tr><td><span>InferSent</span></td><td style='text-align:center;' ><span>Sentences</span></td><td style='text-align:center;' ><span>/</span></td><td style='text-align:center;' ><span>English</span></td><td style='text-align:center;' ><span>biLSTM</span></td><td style='text-align:center;' ><span>(Conneau </span><em><span>et al.</span></em><span>, 2018) [Facebook]</span></td></tr><tr><td><span>ELMo</span></td><td style='text-align:center;' ><span>Words</span></td><td style='text-align:center;' ><span>Dynamic</span></td><td style='text-align:center;' ><span>English</span></td><td style='text-align:center;' ><span>biLSTM</span></td><td style='text-align:center;' ><span>(Peters </span><em><span>et al.</span></em><span>, 2018) [AllenNLP]</span></td></tr><tr><td><span>BERT</span></td><td style='text-align:center;' ><span>sub-words</span></td><td style='text-align:center;' ><span>Dynamic</span></td><td style='text-align:center;' ><span>English</span></td><td style='text-align:center;' ><span>bidirectional Transformers</span></td><td style='text-align:center;' ><span>(Devlin </span><em><span>et al.</span></em><span>, 2019) [Google]</span></td></tr><tr><td><span>SBERT</span></td><td style='text-align:center;' ><span>Sentences</span></td><td style='text-align:center;' ><span>/</span></td><td style='text-align:center;' ><span>Multiple languages</span></td><td style='text-align:center;' ><span>Transformers</span></td><td style='text-align:center;' ><span>(Reimers and Gurevych, 2019)</span></td></tr><tr><td><span>SciBERT</span></td><td style='text-align:center;' ><span>Sub-words</span></td><td style='text-align:center;' ><span>Dynamic</span></td><td style='text-align:center;' ><span>English</span></td><td style='text-align:center;' ><span>bidirectional Transformers</span></td><td style='text-align:center;' ><span>(Beltagy, Lo and Cohan, 2019) [AllenNLP]</span></td></tr><tr><td><span>GPT-3</span></td><td style='text-align:center;' ><span>Sentences</span></td><td style='text-align:center;' ><span>/</span></td><td style='text-align:center;' ><span>Multiple languages (API)</span></td><td style='text-align:center;' ><span>Transformers</span></td><td style='text-align:center;' ><span>(Brown </span><em><span>et al.</span></em><span>, 2020) [OpenAI]</span></td></tr></tbody></table></figure><p><span>I started this comparison with the classical methods for calculating Embeddings, for instance: The </span><strong><span>Bag Of Words</span></strong><span> (</span><strong><span>BOW</span></strong><span>) and the </span><strong><span>TF-IDF</span></strong><span> approaches. These methods are very popular methods and very easy to implement. But they are too simple; Don’t work on a semantic level; Bad accuracy compared to other methods; Need to be fine-tuned then trained on the corpus (No pre-trained models).</span></p><p><span>I found next, the </span><strong><span>Words to Vectors</span></strong><span> (</span><strong><span>Word2Vec</span></strong><span>) model with its two variants (</span><strong><span>CBOW</span></strong><span> and </span><strong><span>Skip-Gram</span></strong><span>). Both the variants of Word2Vec are based on </span><em><span>Feed Forward Neural Networks</span></em><span> (</span><em><span>FNNs</span></em><span>), they differ only in the mission. While CBOW tries to predict a word given the surrounding words. The Skip-Gram model tries to predict the surrounding words given one word. This difference leads to a little change in the characteristics of the word vectors: For example, the CBOW is characterized by its speed while Skip-Gram gives good results for rare words. The downside of these models is that they don’t capture polysemy.</span></p><p><span>The next option is the </span><strong><span>Global Vectors</span></strong><span> model (</span><strong><span>GloVe</span></strong><span>) which is a </span><em><span>probabilistic model</span></em><span> based on global word-word co-occurrence statistics that learn static word representations from the corpus. This model is known for its ease of parallelization but it is also memory-intensive in the training phase. So If you are limited in resources (RAM and GPU), it is only useful if you use a pre-trained model.</span></p><p><span>My next option on the list is </span><strong><span>Documents to vectors</span></strong><span> (</span><strong><span>Doc2Vec</span></strong><span>). This method was proposed by Google, it uses a simple </span><em><span>Feed Forward Neural Network</span></em><span> (</span><em><span>FNN</span></em><span>) to calculate the embedding of a sentence directly. This model uses the same architecture as Word2Vec but with a little change that made a great improvement in understanding the sentences (Added a special token that holds the meaning of the sentence). An implementation is provided in the Gensim library with a pre-trained model for the English language. This model is better than the previously mentioned models in the document scale.</span></p><p><span>Another model that could accomplish the embedding task is </span><strong><span>Skip-Thought</span></strong><span> which is a simple </span><em><span>LSTM</span></em><span> model for learning fixed-length representations of sentences. This model is accurate in calculating the semantic similarity between sentences and for classification tasks. It is available also as a pre-trained model which made it a good choice for sentence embedding tasks.</span></p><p><span>The next option in this list is another implementation of </span><strong><span>Word2Vec</span></strong><span> model based on character N-Grams. It is available at the open-source library </span><em><span>FastText</span></em><span> developed by Facebook. This model is better than the other Word2Vec models as it is good in capturing rare or out-of-vocabulary words. It is also good to mention that lot of pre-trained models are available (Provided by Facebook) for English and for non-English text trained on Wikipedia data. But at a sentence level, it is worse than sentence-based models as it is based on N-Grams.</span></p><p><span>The </span><strong><span>Universal Sentence Encoder</span></strong><span> (or </span><strong><span>USE</span></strong><span> for short) is a very powerful model in this list. It has two variants that are both developed by Google: (I) The first one is using a </span><em><span>Transformer</span></em><span> architecture. This technique performs better at the cost of computation time as the network is trained on multiple tasks; (II) The second one is using </span><em><span>Deep Averaging Network (DAN)</span></em><span> which is way faster but less accurate than the first option. It is a great option for sentence embedding use-cases because: (1) Pre-trained models are available in Tensor-flow hub and well documented; (2) Very good accuracy in semantic textual similarity as it is trained for this task particularly. One downside for it is that the code is not published (If you want to train the model you have to code it) and there are missing technical details about the architecture used by the researchers who published it. But if you opt for using a pre-trained model, this will not be an issue for you.</span></p><p><span>The next model is developed by Facebook for sentence embeddings calculation. This model is called </span><strong><span>InferSent</span></strong><span>. It is based on </span><em><span>biLSTM</span></em><span> architecture and is available to download as a pre-trained model for the English language. It is designed for Natural Language Understanding (NLU) tasks, especially for sentence semantic similarity calculation.</span></p><p><span>Another option on the list is </span><strong><span>Embeddings from Language Models</span></strong><span> (</span><strong><span>ELMo</span></strong><span>) which is a word embedding model developed by AllenNLP. It is based on </span><em><span>biLSTM</span></em><span> architecture and is available as a pre-trained model for the English language. It is very good at calculating word representations depending on the context (Dynamic word representations). And it is very good at capturing the polysemy. But it is not sufficient for sentence embeddings.</span></p><p><span>The next option is a very popular and very powerful model that is called </span><strong><span>BERT</span></strong><span> (Stands for </span><strong><span>Bidirectional Encoder Representations from Transformers</span></strong><span>). It was developed by Google using bidirectional Transformers to learn dynamic representations for sub-words. It generates very accurate vectors for words. But it is designed for generating tasks (Natural Language Generation -NLG-). So this model is not adopted for language understanding use-cases but it was the basis for many other models that are useful for these tasks.</span></p><p><span>The next model is </span><strong><span>Sentence BERT</span></strong><span> (</span><strong><span>SBERT</span></strong><span>) which is based on </span><em><span>bidirectional Transformers</span></em><span>. This model is simply BERT designed for sentences. It has a very high accuracy for calculating the semantic textual similarity between paragraphs. There are a lot of pre-trained models in the literature that are based on this model (Using multiple datasets), some of them are specific for the English language, and the others are multilingual, some of them are adapted for NLG and others for NLI. So this model is adapted for multiple situations.</span></p><p><span>Another BERT-based model in this list is </span><strong><span>Scientific BERT</span></strong><span> (</span><strong><span>SciBERT</span></strong><span>) (Developed by AllenNLP). This option is just the BERT model trained on a very large dataset of English scientific papers. This model is very adapted for processing scientific information use-cases. As it could capture the meaning of scientific words and represent them well. But at the same time, it has critical issues for some other use-case. For instance: It is trained for generative tasks so don’t use it for understanding tasks; It is designed for word embeddings rather than sentences.</span></p><p><span>The next option is the state-of-the-art in multiple NLP tasks (NLG or NLU) till the day of writing these words. This model is called </span><strong><span>Generative Pre-trained Transformer 3</span></strong><span> (</span><strong><span>GPT-3</span></strong><span>). It is developed by the team of OpenAI using the </span><em><span>Transformers</span></em><span> architecture for embedding sentences. A pre-trained model exists and is available only through the official API offered by the team (Via the Internet). It outperforms all the other models in almost all NLP tasks. But it is not available for download. In fact, it could be used only as a black-box service. For that reason, if you don’t want to depend on the OpenAI enterprise in case they change the policy of use of the API you should not use it.</span></p><h1 id='a-simple-use-case-for-embeddings'><span>A simple use-case for Embeddings </span></h1><p><span>We want here to implement a simple semantic textual similarity calculator. That compares two scientific articles using the abstracts. To accomplish this task, the most common way is to:</span></p><ol><li><span>Build Text Embeddings</span></li><li><span>Calculate the distance between the vectors resulting from the previous step. This distance could be accomplished by calculating the distance using </span><em><span>Hamming Distance</span></em><span>, </span><em><span>Euclidean Distance</span></em><span>, </span><em><span>Manhattan Distance,</span></em><span> or </span><em><span>Minkowski Distance</span></em><span>. Also, it could be calculated using the </span><em><span>Cosine Similarity</span></em><span> measure which I’ll be using in this article.</span></li></ol><p><span>So what we want here is an Embedding model that:</span></p><ul><li><span>Is based on deep learning because they are more accurate.</span></li><li><span>Embeds Sentences not chars, nor words.</span></li><li><span>Has pre-trained models as we don’t want to train a new model if the quality of existing models is enough.</span></li><li><span>Has at least one implementation that could be used as we don’t want the article to be longer than it is.</span></li><li><span>Is designed for language understanding, not generation.</span></li></ul><p><span>After eliminating methods that are not suitable for our use case, we are left with 4 options: </span><strong><span>Skip-Thought</span></strong><span>, </span><strong><span>USE</span></strong><span>, </span><strong><span>InferSent,</span></strong><span> and </span><strong><span>SBERT</span></strong><span>. The next step is to compare these models on the textual similarity task. To do so we have to search (On this website </span><a href='https://paperswithcode.com/'><span>PapersWithCode</span></a><span> for example) for the goodness of these models on the </span><em><span>STS Benchmark</span></em><span> (</span><em><span>Sentiment Textual Similarity Benchmark</span></em><span>) (The results are shown in the next Table) and we havSentimante found that the </span><strong><span>USE</span></strong><span> model with Transformers architecture gives the best results in this task.</span></p><figure><table><thead><tr><th><span>Model</span></th><th style='text-align:center;' ><span>STS-B</span></th></tr></thead><tbody><tr><td><span>Skip-Thought</span></td><td style='text-align:center;' ><span>0.75</span></td></tr><tr><td><span>USE(Transformers)</span></td><td style='text-align:center;' ><strong><span>0.78</span></strong></td></tr><tr><td><span>USE(DAN)</span></td><td style='text-align:center;' ><span>0.76</span></td></tr><tr><td><span>InferSent</span></td><td style='text-align:center;' ><span>0.77</span></td></tr><tr><td><span>SBERT</span></td><td style='text-align:center;' ><span>0.77</span></td></tr></tbody></table></figure><p><span>After choosing the model, the second step is calculating the distance. As we mentioned earlier we will be using in this article the </span><em><span>Cosine Similarity</span></em><span> function. Our Semantic Textual Similarity Calculator will look something like this:</span></p><p><img src="assets/Models-simentic_similarity_papers.png" style="zoom: 70%;" /></p><ol start='' ><li><span>Get the abstracts of the two papers (</span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="2.44ex" height="1.885ex" role="img" focusable="false" viewBox="0 -683 1078.6 833" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.339ex;"><defs><path id="MJX-10-TEX-I-1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path><path id="MJX-10-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D443" xlink:href="#MJX-10-TEX-I-1D443"></use></g><g data-mml-node="mn" transform="translate(675,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-10-TEX-N-31"></use></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>P</mi><mn>1</mn></msub></math></mjx-assistive-mml></mjx-container><script type="math/tex">P_1</script><span> and </span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="2.44ex" height="1.885ex" role="img" focusable="false" viewBox="0 -683 1078.6 833" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.339ex;"><defs><path id="MJX-11-TEX-I-1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path><path id="MJX-11-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D443" xlink:href="#MJX-11-TEX-I-1D443"></use></g><g data-mml-node="mn" transform="translate(675,-150) scale(0.707)"><use data-c="32" xlink:href="#MJX-11-TEX-N-32"></use></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>P</mi><mn>2</mn></msub></math></mjx-assistive-mml></mjx-container><script type="math/tex">P_2</script><span>).</span></li><li><span>Use the </span><strong><span>USE</span></strong><span> pre-trained model to generate the Embeddings of these papers. (</span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="2.44ex" height="3.853ex" role="img" focusable="false" viewBox="0 -1553 1078.6 1703" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.339ex;"><defs><path id="MJX-12-TEX-I-1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path><path id="MJX-12-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-12-TEX-S4-2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"></path><path id="MJX-12-TEX-S4-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D443" xlink:href="#MJX-12-TEX-I-1D443"></use></g><g data-mml-node="mn" transform="translate(675,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-12-TEX-N-31"></use></g></g><g data-mml-node="mo" transform="translate(0,870)"><use data-c="2192" xlink:href="#MJX-12-TEX-S4-2192" transform="translate(78.6,0)"></use><svg width="178.6" height="865" x="0" y="-182" viewBox="44.6 -182 178.6 865"><use data-c="2212" xlink:href="#MJX-12-TEX-S4-2212" transform="scale(0.344,1)"></use></svg></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover><msub><mi>P</mi><mn>1</mn></msub><mo>→</mo></mover></math></mjx-assistive-mml></mjx-container><script type="math/tex">\overrightarrow{P_1}</script><span> and </span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="2.44ex" height="3.853ex" role="img" focusable="false" viewBox="0 -1553 1078.6 1703" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -0.339ex;"><defs><path id="MJX-13-TEX-I-1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path><path id="MJX-13-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-13-TEX-S4-2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"></path><path id="MJX-13-TEX-S4-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D443" xlink:href="#MJX-13-TEX-I-1D443"></use></g><g data-mml-node="mn" transform="translate(675,-150) scale(0.707)"><use data-c="32" xlink:href="#MJX-13-TEX-N-32"></use></g></g><g data-mml-node="mo" transform="translate(0,870)"><use data-c="2192" xlink:href="#MJX-13-TEX-S4-2192" transform="translate(78.6,0)"></use><svg width="178.6" height="865" x="0" y="-182" viewBox="44.6 -182 178.6 865"><use data-c="2212" xlink:href="#MJX-13-TEX-S4-2212" transform="scale(0.344,1)"></use></svg></g></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mover><msub><mi>P</mi><mn>2</mn></msub><mo>→</mo></mover></math></mjx-assistive-mml></mjx-container><script type="math/tex">\overrightarrow{P_2}</script><span>)</span></li><li><span>Calculate the Cosine similarity between vectors of the two papers using this function </span><mjx-container class="MathJax" jax="SVG" style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="16.015ex" height="5.662ex" role="img" focusable="false" viewBox="0 -1553 7078.4 2502.5" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" style="vertical-align: -2.148ex;"><defs><path id="MJX-14-TEX-N-63" d="M370 305T349 305T313 320T297 358Q297 381 312 396Q317 401 317 402T307 404Q281 408 258 408Q209 408 178 376Q131 329 131 219Q131 137 162 90Q203 29 272 29Q313 29 338 55T374 117Q376 125 379 127T395 129H409Q415 123 415 120Q415 116 411 104T395 71T366 33T318 2T249 -11Q163 -11 99 53T34 214Q34 318 99 383T250 448T370 421T404 357Q404 334 387 320Z"></path><path id="MJX-14-TEX-N-6F" d="M28 214Q28 309 93 378T250 448Q340 448 405 380T471 215Q471 120 407 55T250 -10Q153 -10 91 57T28 214ZM250 30Q372 30 372 193V225V250Q372 272 371 288T364 326T348 362T317 390T268 410Q263 411 252 411Q222 411 195 399Q152 377 139 338T126 246V226Q126 130 145 91Q177 30 250 30Z"></path><path id="MJX-14-TEX-N-73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z"></path><path id="MJX-14-TEX-N-2061" d=""></path><path id="MJX-14-TEX-S3-28" d="M701 -940Q701 -943 695 -949H664Q662 -947 636 -922T591 -879T537 -818T475 -737T412 -636T350 -511T295 -362T250 -186T221 17T209 251Q209 962 573 1361Q596 1386 616 1405T649 1437T664 1450H695Q701 1444 701 1441Q701 1436 681 1415T629 1356T557 1261T476 1118T400 927T340 675T308 359Q306 321 306 250Q306 -139 400 -430T690 -924Q701 -936 701 -940Z"></path><path id="MJX-14-TEX-N-2220" d="M71 0L68 2Q65 3 63 5T58 11T55 20Q55 22 57 28Q67 43 346 361Q397 420 474 508Q595 648 616 671T647 694T661 688T666 674Q666 668 663 663Q662 662 627 622T524 503T390 350L120 41L386 40H653Q666 30 666 20Q666 8 651 0H71Z"></path><path id="MJX-14-TEX-N-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path id="MJX-14-TEX-I-1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path><path id="MJX-14-TEX-N-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path id="MJX-14-TEX-S4-2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"></path><path id="MJX-14-TEX-S4-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path><path id="MJX-14-TEX-N-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path id="MJX-14-TEX-N-32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path><path id="MJX-14-TEX-S3-29" d="M34 1438Q34 1446 37 1448T50 1450H56H71Q73 1448 99 1423T144 1380T198 1319T260 1238T323 1137T385 1013T440 864T485 688T514 485T526 251Q526 134 519 53Q472 -519 162 -860Q139 -885 119 -904T86 -936T71 -949H56Q43 -949 39 -947T34 -937Q88 -883 140 -813Q428 -430 428 251Q428 453 402 628T338 922T245 1146T145 1309T46 1425Q44 1427 42 1429T39 1433T36 1436L34 1438Z"></path><path id="MJX-14-TEX-N-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><use data-c="63" xlink:href="#MJX-14-TEX-N-63"></use><use data-c="6F" xlink:href="#MJX-14-TEX-N-6F" transform="translate(444,0)"></use><use data-c="73" xlink:href="#MJX-14-TEX-N-73" transform="translate(944,0)"></use></g><g data-mml-node="mo" transform="translate(1338,0)"><use data-c="2061" xlink:href="#MJX-14-TEX-N-2061"></use></g><g data-mml-node="mrow" transform="translate(1504.7,0)"><g data-mml-node="mo" transform="translate(0 -0.5)"><use data-c="28" xlink:href="#MJX-14-TEX-S3-28"></use></g><g data-mml-node="mi" transform="translate(736,0)"><use data-c="2220" xlink:href="#MJX-14-TEX-N-2220"></use></g><g data-mml-node="mo" transform="translate(1458,0)"><use data-c="28" xlink:href="#MJX-14-TEX-N-28"></use></g><g data-mml-node="mover" transform="translate(1847,0)"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D443" xlink:href="#MJX-14-TEX-I-1D443"></use></g><g data-mml-node="mn" transform="translate(675,-150) scale(0.707)"><use data-c="31" xlink:href="#MJX-14-TEX-N-31"></use></g></g><g data-mml-node="mo" transform="translate(0,870)"><use data-c="2192" xlink:href="#MJX-14-TEX-S4-2192" transform="translate(78.6,0)"></use><svg width="178.6" height="865" x="0" y="-182" viewBox="44.6 -182 178.6 865"><use data-c="2212" xlink:href="#MJX-14-TEX-S4-2212" transform="scale(0.344,1)"></use></svg></g></g><g data-mml-node="mo" transform="translate(2925.6,0)"><use data-c="2C" xlink:href="#MJX-14-TEX-N-2C"></use></g><g data-mml-node="mover" transform="translate(3370.2,0)"><g data-mml-node="msub"><g data-mml-node="mi"><use data-c="1D443" xlink:href="#MJX-14-TEX-I-1D443"></use></g><g data-mml-node="mn" transform="translate(675,-150) scale(0.707)"><use data-c="32" xlink:href="#MJX-14-TEX-N-32"></use></g></g><g data-mml-node="mo" transform="translate(0,870)"><use data-c="2192" xlink:href="#MJX-14-TEX-S4-2192" transform="translate(78.6,0)"></use><svg width="178.6" height="865" x="0" y="-182" viewBox="44.6 -182 178.6 865"><use data-c="2212" xlink:href="#MJX-14-TEX-S4-2212" transform="scale(0.344,1)"></use></svg></g></g><g data-mml-node="mo" transform="translate(4448.8,0) translate(0 -0.5)"><use data-c="29" xlink:href="#MJX-14-TEX-S3-29"></use></g></g><g data-mml-node="mo" transform="translate(6689.4,0)"><use data-c="29" xlink:href="#MJX-14-TEX-N-29"></use></g></g></g></svg><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>cos</mi><mo data-mjx-texclass="NONE">⁡</mo><mrow data-mjx-texclass="INNER"><mo data-mjx-texclass="OPEN">(</mo><mi mathvariant="normal">∠</mi><mo stretchy="false">(</mo><mover><msub><mi>P</mi><mn>1</mn></msub><mo>→</mo></mover><mo>,</mo><mover><msub><mi>P</mi><mn>2</mn></msub><mo>→</mo></mover><mo data-mjx-texclass="CLOSE">)</mo></mrow><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container><script type="math/tex">\cos(\angle (\overrightarrow{P_1}, \overrightarrow{P_2}))</script></li></ol><h1 id='conclusion'><span>Conclusion</span></h1><p><span>We have seen in this article the importance of text embeddings in the field of NLP. We have learned how to choose a good model that fits our needs from all the models existing on the literature using a simple procedure which is:</span></p><ol><li><span>Clearly identify your needs, using a series of specific questions.</span></li><li><span>Filter the list of all available Embedding methods based on your answers.</span></li><li><span>Compare the remaining methods based on a benchmark that focuses on your needs (Benchmark for semantic Similarity, Sentiment Analysis, Question Answering, Text Comprehension, …).</span></li></ol><p><span>We have seen also a small demo on how to use Embeddings to calculate the semantic similarity between some scientific articles using the </span><strong><span>USE</span></strong><span> model and the </span><em><span>Cosine Similarity</span></em><span> function.</span></p><h1 id='references'><span>References</span></h1><ul><li><span>Harris, Z.S. (1954) ‘Distributional structure’, </span><em><span>Word</span></em><span>, 10(2–3), pp. 146–162.</span></li><li><span>Jones, K.S. (1972) ‘A statistical interpretation of term specificity and its application in retrieval’, </span><em><span>Journal of documentation</span></em><span> [Preprint].</span></li><li><span>Mikolov, T. </span><em><span>et al.</span></em><span> (2013) ‘Efficient Estimation of Word Representations in Vector Space’, </span><em><span>arXiv:1301.3781 [cs]</span></em><span> [Preprint]. Available at: </span><a href='http://arxiv.org/abs/1301.3781' target='_blank' class='url'>http://arxiv.org/abs/1301.3781</a><span> (Accessed: 23 May 2021).</span></li><li><span>Pennington, J., Socher, R. and Manning, C. (2014) ‘Glove: Global Vectors for Word Representation’, in </span><em><span>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</span></em><span>. </span><em><span>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</span></em><span>, Doha, Qatar: Association for Computational Linguistics, pp. 1532–1543. doi:</span><a href='https://doi.org/10.3115/v1/D14-1162'><span>10.3115/v1/D14-1162</span></a><span>.</span></li><li><span>Le, Q.V. and Mikolov, T. (2014) ‘Distributed Representations of Sentences and Documents’, </span><em><span>arXiv:1405.4053 [cs]</span></em><span> [Preprint]. Available at: </span><a href='http://arxiv.org/abs/1405.4053' target='_blank' class='url'>http://arxiv.org/abs/1405.4053</a><span> (Accessed: 23 May 2021).</span></li><li><span>Kiros, R. </span><em><span>et al.</span></em><span> (2015) ‘Skip-Thought Vectors’, </span><em><span>arXiv:1506.06726 [cs]</span></em><span> [Preprint]. Available at: </span><a href='http://arxiv.org/abs/1506.06726' target='_blank' class='url'>http://arxiv.org/abs/1506.06726</a><span> (Accessed: 18 August 2021).</span></li><li><span>Joulin, A. </span><em><span>et al.</span></em><span> (2016) ‘Bag of Tricks for Efficient Text Classification’, </span><em><span>arXiv:1607.01759 [cs]</span></em><span> [Preprint]. Available at: </span><a href='http://arxiv.org/abs/1607.01759' target='_blank' class='url'>http://arxiv.org/abs/1607.01759</a><span> (Accessed: 23 May 2021).</span></li><li><span>Cer, D. </span><em><span>et al.</span></em><span> (2018) ‘Universal Sentence Encoder’, </span><em><span>arXiv:1803.11175 [cs]</span></em><span> [Preprint]. Available at: </span><a href='http://arxiv.org/abs/1803.11175' target='_blank' class='url'>http://arxiv.org/abs/1803.11175</a><span> (Accessed: 18 May 2021).</span></li><li><span>Conneau, A. </span><em><span>et al.</span></em><span> (2018) ‘Supervised Learning of Universal Sentence Representations from Natural Language Inference Data’, </span><em><span>arXiv:1705.02364 [cs]</span></em><span> [Preprint]. Available at: </span><a href='http://arxiv.org/abs/1705.02364' target='_blank' class='url'>http://arxiv.org/abs/1705.02364</a><span> (Accessed: 23 May 2021).</span></li><li><span>Peters, M.E. </span><em><span>et al.</span></em><span> (2018) ‘Deep contextualized word representations’, </span><em><span>arXiv:1802.05365 [cs]</span></em><span> [Preprint]. Available at: </span><a href='http://arxiv.org/abs/1802.05365' target='_blank' class='url'>http://arxiv.org/abs/1802.05365</a><span> (Accessed: 23 May 2021).</span></li><li><span>Devlin, J. </span><em><span>et al.</span></em><span> (2019) ‘BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding’, </span><em><span>arXiv:1810.04805 [cs]</span></em><span> [Preprint]. Available at: </span><a href='http://arxiv.org/abs/1810.04805' target='_blank' class='url'>http://arxiv.org/abs/1810.04805</a><span> (Accessed: 23 May 2021).</span></li><li><span>Reimers, N. and Gurevych, I. (2019) ‘Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks’, </span><em><span>arXiv:1908.10084 [cs]</span></em><span> [Preprint]. Available at: </span><a href='http://arxiv.org/abs/1908.10084' target='_blank' class='url'>http://arxiv.org/abs/1908.10084</a><span> (Accessed: 21 March 2021).</span></li><li><span>Beltagy, I., Lo, K. and Cohan, A. (2019) ‘SciBERT: A Pretrained Language Model for Scientific Text’, </span><em><span>arXiv:1903.10676 [cs]</span></em><span> [Preprint]. Available at: </span><a href='http://arxiv.org/abs/1903.10676' target='_blank' class='url'>http://arxiv.org/abs/1903.10676</a><span> (Accessed: 23 May 2021).</span></li><li><span>Brown, T.B. </span><em><span>et al.</span></em><span> (2020) ‘Language Models are Few-Shot Learners’, </span><em><span>arXiv:2005.14165 [cs]</span></em><span> [Preprint]. Available at: </span><a href='http://arxiv.org/abs/2005.14165' target='_blank' class='url'>http://arxiv.org/abs/2005.14165</a><span> (Accessed: 23 May 2021).</span></li></ul></div></div>
<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    
    var disqus_config = function () {
    this.page.url = "http://mehdi-chebbah.ml/Embedding-Methods-Comparison/";  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = "Embedding-Methods-Comparison"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://mehdi-chebbah.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</body>
</html>

<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<link rel="stylesheet" href="../../css/blog-post.css" type="text/css">

<title>LDA, NMF, Top2Vec, and BERTopic. How do they work?
</title>

<meta name="description" content="In this article, we will go through the literature on Topic Modeling. We will present an overview of the most interesting techniques, explain how they work, and extract their characteristics that play an important role when choosing the right model for your problem.">
<meta name="robots" content="index, follow">
<meta property="og:type" content="article" />
<meta property="og:title" content="LDA, NMF, Top2Vec, and BERTopic. How do they work?" />
<meta property="og:description" content="In this article, we will go through the literature on Topic Modeling. We will present an overview of the most interesting techniques, explain how they work, and extract their characteristics that play an important role when choosing the right model for your problem." />
<meta property="og:site_name" content="Mehdi CHEBBAH | Blog" />

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    });
</script>
    
<script type="text/javascript"
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>
<body class='typora-export'><div class='typora-export-content'>
<div id='write'  class=''><div style="text-align:center;font-size:30px">﷽</div><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p><div style="text-align:center;font-size:48px">LDA, NMF, Top2Vec, and BERTopic. How do they work?</div><p>&nbsp;</p><p>&nbsp;</p><p><span>Written by </span><strong><span>CHEBBAH Mehdi</span></strong></p><hr /><p><img src="./assets/Cover.jpg" referrerpolicy="no-referrer"></p><p>&nbsp;</p><h1 id='introduction'><span>Introduction</span></h1><p><em><span>Topic modeling</span></em><span> (or </span><em><span>topic extraction</span></em><span>) is a technique in Natural Language Processing (NLP) that allows the machine to extract meaning from text by identifying recurrent abstract themes or topics represented by the most relevant keywords.</span></p><p><span>In this article, we will go through the literature on Topic Modeling. We will present an overview of the most interesting techniques, explain how they work, and extract their characteristics that play an important role when choosing the right model for your problem.</span></p><h2 id='lda'><span>LDA</span></h2><p><strong><span>LDA</span></strong><span> or </span><strong><span>Latent Dirichlet Allocation</span></strong><span> is a probabilistic model proposed by </span><em><span>Pritchard</span></em><span>, </span><em><span>Stephens</span></em><span>, and </span><em><span>Donnelly</span></em><span> in 2000. Then it was applied in machine learning by </span><em><span>Blei David</span></em><span>, </span><em><span>Ng Andrew</span></em><span>, and </span><em><span>Jordan Michael</span></em><span> in 2003. </span></p><p><span>Technically, a document in </span><strong><span>LDA</span></strong><span> is a probability distribution of topics and a topic in its turn is a probability distribution of words, so in the training phase, </span><strong><span>LDA</span></strong><span> model tries to find -in the same time- the best distribution of topics in each Document, and the best distribution of words in each topic that describe better the dataset. And this is how it does that:</span></p><p><img src="assets/LDA.png"  /></p><ul><li><span>The model takes two inputs: </span><strong><span>The Document-Term Matrix</span></strong><span> (The order of words is ignored in LDA so the input is </span><strong><span>BOW: Bag Of Words</span></strong><span>) and </span><strong><span>the Number of Topics $K$</span></strong><span> this number is hard to detect beforehand, so this hyper-parameter have to be fine-tuned.</span></li></ul><ol start='2' ><li><p><span>Assign each word in each document to a random topic which will give a </span><strong><span>Bad</span></strong><span> distribution of topics over documents and a </span><strong><span>Bad</span></strong><span> distribution of words over topics.</span></p></li><li><p><span>For each document $d_i$ and for each Word $w_j$, calculate $P(w_j | t_k)$ and $P(t_k|d_i)$ for each topic $t_{k=1,2,..,K}$.</span></p><p><strong><span>Note</span></strong><span>: In some implementations of LDA, two density factors are introduced in the calculation of these probabilities: a document-topic density factor $\alpha$ and a topic-word density factor $\beta$ which are two other hyper-parameters for the model.</span></p></li><li><p><span>Re-assign each word to a topic where  $P(w_j | t_k) \times P(t_k|d_i)$ is maximum.</span></p></li><li><p><span>Repeat steps 3 and 4 until the model converges or to a pre-defined number of iterations.</span></p></li></ol><ul><li><span>The output of the model is the distribution of the topics over documents and the distribution of words over topics</span></li></ul><h3 id='properties-1'><span>Properties</span></h3><ul><li><span>Consider a document as a mixture of topics.</span></li><li><span>Works better with long texts.</span></li><li><span>A non-deterministic approach.</span></li><li><span>Language agnostic.</span></li></ul><h3 id='nmf'><span>NMF</span></h3><p><strong><span>NMF</span></strong><span> or </span><strong><span>Non-negative Matrix Factorization</span></strong><span> is a Linear-algebraic model. The basic idea behind it is to try to factories the input $A$ -which is the words-documents matrix- to get two other matrices (words-topics matrix $W$ and topics-documents matrix $H$) where $A = W \times H$. The algorithm for calculating these matrices is described below:</span></p><p><img src="assets/NMF.png" referrerpolicy="no-referrer" alt="NMF pic"></p><ul><li><span>The input for this model is the words-documents matrix $A$ of dimension $(n\times m)$ and the number of topics $K$.</span></li></ul><ol start='' ><li><span>Randomly initialize the two matrices $W$ of dimension $(n \times K)$ and $H$ of dimension $(K \times m)$.</span></li><li><span>Reconstruct the matrix $\hat{A} = W \times H$.</span></li><li><span>Calculate the distance between $A$ and $\hat{A}$ (euclidean distance).</span></li><li><span>Update values in $W$ and $H$ based on an objective function.</span></li><li><span>Repeat steps 2, 3, and 4 until convergence.</span></li></ol><ul><li><span>The output of the model is $W$ and $H$ matrices.</span></li></ul><h3 id='properties-2'><span>Properties</span></h3><ul><li><span>Calculates how well each document fits each topic.</span></li><li><span>Usually faster than LDA.</span></li><li><span>Works better with brief texts.</span></li><li><span>A deterministic approach.</span></li><li><span>Language agnostic.</span></li></ul><h3 id='top2vec'><span>Top2Vec</span></h3><p><strong><span>Top2Vec</span></strong><span> is an algorithm for topic modeling and semantic search. It automatically detects topics present in text and generates jointly embedded topic, document and word vectors. </span><strong><span>Top2Vec</span></strong><span> was first published in arXiv by </span><em><span>Dimo Angelov</span></em><span> in 2020. It works as follows:</span></p><p><img src="assets/Top2Vec.png" referrerpolicy="no-referrer"></p><ul><li><span>The model takes the collection of documents only as an input.</span></li></ul><ol start='' ><li><span>Embed the inputs using an Embedding model (There are three options for accomplishing this task: </span><strong><span>Universal Sentence Encoder (USE)</span></strong><span>, </span><strong><span>Sentence BERT (SBERT)</span></strong><span>, and </span><strong><span>Document to Vector (Doc2Vec)</span></strong><span>.</span></li><li><span>Reduce the dimensionality of the Embeddings space using the </span><strong><span>UMAP</span></strong><span> algorithm to create dense areas. These dense areas contain geometrically close words (Semantically close Words).</span></li><li><span>Cluster the results using a density-based clustering algorithm (The </span><strong><span>HDBSCAN</span></strong><span> is used here). Then calculate the centroids of each cluster (These centroids are considered as the representative vector of the topic).</span></li><li><span>Get the $K$ closest words to the center of each cluster using the </span><strong><span>K-NN</span></strong><span> algorithm (These words represent the keywords of each topic). </span></li></ol><ul><li><span>The outputs of this model are the list of topics and keywords and the trained model that could be used later to cluster new documents into the right topic.</span></li></ul><h3 id='properties-3'><span>Properties</span></h3><ul><li><span>Training is slow.</span></li><li><span>Automatically finds the number of topics.</span></li><li><span>Works on brief text.</span></li><li><span>Language agnostic if used with doc2vec embedding technique.</span></li><li><span>A non-deterministic approach.</span></li><li><span>Bad with few number of documents.</span></li></ul><h3 id='bertopic'><span>BERTopic</span></h3><p><strong><span>BERTopic</span></strong><span> is a topic modeling technique that leverages transformers and c-TF-IDF to create dense clusters allowing for easily interpretable topics whilst keeping important words in the topic descriptions. </span><strong><span>BERTopic</span></strong><span> is the newest topic modeling technique in the list, it was published in 2020 by </span><em><span>Maarten Grootendorst</span></em><span>. Here how it works:</span></p><p><img src="assets/BERTopic.png" referrerpolicy="no-referrer"></p><ul><li><span>It only takes one input, which is the collection of documents.</span></li></ul><ol start='' ><li><p><span>Embed documents using a BERT-based embedding model (Choose the right Transformers-based pre-trained model for your language as this approach doesn’t contain a training neither a fine-tuning phase for the embedding model)</span></p></li><li><p><span>Cluster documents by reducing the dimensionality </span></p><ol start='' ><li><span>Lower the dimensionality of the Embeddings using the </span><strong><span>UMAP</span></strong><span> algorithm.</span></li><li><span>Create clusters using a density-based algorithm such </span><strong><span>HDBSCAN</span></strong><span>.</span></li></ol></li><li><p><span>Create topics from these clusters, by:</span></p><ol start='' ><li><span>Apply a modified version of TF-IDF called class-based TF-IDF (c-TF-IDF) . This version relies on the idea that instead of calculating the importance of words in documents (which is done by TF-IDF) we calculate the importance of words in clusters. Which will allow us to get the most important words in each topic (Representative words).</span></li><li><span>Improve the coherence of words in each topic, using the </span><strong><span>Maximal Marginal Relevance (MMR)</span></strong><span> to find the most coherent words without having too much overlap between the words themselves. This results in the removal of words that do not contribute to a topic.</span></li></ol></li></ol><ul><li><span>The results are the list of topics and top keywords in each topic besides the model that could be used later for extracting topics from new documents.</span></li></ul><h3 id='properties-4'><span>Properties</span></h3><ul><li><span>Training is slow.</span></li><li><span>Automatically finds the number of topics.</span></li><li><span>Works on brief text.</span></li><li><span>A non-deterministic approach.</span></li><li><span>Language agnostic (Any Transformers-based embedding model could be used for embedding phase).</span></li><li><span>Bad with few number of documents.</span></li></ul><p>&nbsp;</p><h1 id='conclusion'><span>Conclusion</span></h1><p><span>These were 4 methods for topic modeling: </span></p><ul><li><span>A probabilistic model: LDA</span></li><li><span>An algebric model: NMF</span></li><li><span>A hybrid approach: Top2Vec, BERTopic</span></li></ul><p><span>If you are intersted in other articles like this, visit me on:</span></p><ul><li><span>My website: </span><a href='https://mehdi-chebbah.ml/' target='_blank' class='url'>https://mehdi-chebbah.ml/</a></li><li><span>my Blog: </span><a href='http://mehdi-chebbah.ml/blog/' target='_blank' class='url'>http://mehdi-chebbah.ml/blog/</a></li><li><span>Medium: </span><a href='https://medium.com/@mehdi_chebbah' target='_blank' class='url'>https://medium.com/@mehdi_chebbah</a></li><li><span>LinkedIn: </span><a href='https://www.linkedin.com/in/mehdi-chebbah/' target='_blank' class='url'>https://www.linkedin.com/in/mehdi-chebbah/</a></li><li><span>Twitter: </span><a href='https://twitter.com/MehdiCHEBBAH1' target='_blank' class='url'>https://twitter.com/MehdiCHEBBAH1</a></li></ul><h1 id='references'><span>References</span></h1><ul><li><a href='http://genetics.org/content/155/2/945' target='_blank' class='url'>http://genetics.org/content/155/2/945</a></li><li><a href='https://jmlr.csail.mit.edu/papers/v3/blei03a.html' target='_blank' class='url'>https://jmlr.csail.mit.edu/papers/v3/blei03a.html</a></li><li><a href='https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#cite_note-blei2003-3' target='_blank' class='url'>https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation#cite_note-blei2003-3</a></li><li><a href='https://medium.com/ml2vec/topic-modeling-is-an-unsupervised-learning-approach-to-clustering-documents-to-discover-topics-fdfbf30e27df' target='_blank' class='url'>https://medium.com/ml2vec/topic-modeling-is-an-unsupervised-learning-approach-to-clustering-documents-to-discover-topics-fdfbf30e27df</a></li><li><a href='https://www.researchgate.net/figure/Conceptual-illustration-of-non-negative-matrix-factorization-NMF-decomposition-of-a_fig1_312157184' target='_blank' class='url'>https://www.researchgate.net/figure/Conceptual-illustration-of-non-negative-matrix-factorization-NMF-decomposition-of-a_fig1_312157184</a></li><li><a href='https://github.com/MaartenGr/BERTopic' target='_blank' class='url'>https://github.com/MaartenGr/BERTopic</a></li><li><a href='https://doi.org/10.5281/zenodo.4381785' target='_blank' class='url'>https://doi.org/10.5281/zenodo.4381785</a></li><li><a href='https://top2vec.readthedocs.io/en/latest/Top2Vec.html' target='_blank' class='url'>https://top2vec.readthedocs.io/en/latest/Top2Vec.html</a></li><li><a href='https://arxiv.org/abs/2008.09470' target='_blank' class='url'>https://arxiv.org/abs/2008.09470</a></li></ul></div></div>
<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    
    var disqus_config = function () {
    this.page.url = "http://mehdi-chebbah.ml/topic-modeling-algorithms/";  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = "topic-modeling-algorithms"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://mehdi-chebbah.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
      
  </body>
</html>